# NewsPipeline
ğŸ“° NewsPipeline - ETL de flux dâ€™actualitÃ©s basÃ© sur Kafka et Docker
ğŸš€ PrÃ©sentation
NewsPipeline est une solution ETL conteneurisÃ©e permettant de rÃ©cupÃ©rer, transformer et stocker des donnÃ©es issues de l'API News API. Elle repose sur une architecture microservices orchestrÃ©e via Docker Compose, et utilise Apache Kafka pour la gestion de flux de donnÃ©es en temps rÃ©el.

Le but est de permettre une collecte automatisÃ©e et scalable de donnÃ©es dâ€™actualitÃ©s, avec un traitement asynchrone et une persistance dans une base MongoDB pour des analyses ultÃ©rieures.

ğŸ› ï¸ Architecture
lua
Copier
Modifier
+-------------+          +------------+         +-------------+         +-------------+
|  Aggregator | â”€â”€â”€â”€â”€â”€â”€â–º | Kafka Pub  | â”€â”€â”€â”€â”€â–º  | Kafka Topic | â”€â”€â”€â”€â”€â–º  | Topic-sub   |
| (Data Fetch)|          | (Producer) |         | (Stream)    |         | (Consumer)  |
+-------------+          +------------+         +-------------+         +-------------+
                                                                              â”‚
                                                                              â–¼
                                                                       +-------------+
                                                                       |   MongoDB   |
                                                                       +-------------+
Description des composants
Aggregator : RÃ©cupÃ¨re les donnÃ©es depuis lâ€™API News API et les structure selon un modÃ¨le prÃ©dÃ©fini.

Kafka Publisher (AggCloudKafka) : Publie les donnÃ©es formatÃ©es sur des topics Kafka spÃ©cifiques.

Kafka : GÃ¨re les flux de messages entre les producteurs et consommateurs. Assure la fiabilitÃ© et la scalabilitÃ©.

Topic-sub (Consumer) : Consomme les donnÃ©es Kafka et les enregistre dans MongoDB.

MongoDB : Stocke les donnÃ©es persistÃ©es, prÃªtes pour lâ€™analyse ou la visualisation.

Tous les composants sont conteneurisÃ©s Ã  lâ€™aide de Docker et orchestrÃ©s via Docker Compose.

âš™ï¸ Technologies utilisÃ©es
ğŸ³ Docker / Docker Compose

ğŸ” Apache Kafka

ğŸ“¡ News API (https://newsapi.org)

ğŸ§© Python (scripts dâ€™ETL)

ğŸ—„ï¸ MongoDB

ğŸ§± Microservices architecture

ğŸ“¦ Installation et dÃ©ploiement
1. PrÃ©-requis
Docker : Installation officielle

Docker Compose : Guide dâ€™installation

2. Cloner le projet
bash
Copier
Modifier
git clone https://github.com/ton-repo/news-pipeline.git
cd news-pipeline
3. Configuration
CrÃ©er un fichier .env Ã  la racine avec :

env
Copier
Modifier
NEWS_API_KEY=your_api_key
MONGO_URI=mongodb://mongo:27017/news
VÃ©rifier les paramÃ¨tres de connexion dans les fichiers docker-compose.yml et config/*.json.

4. Lancer le projet
bash
Copier
Modifier
docker-compose up --build
Cela lancera :

Kafka + Zookeeper

MongoDB

Aggregator

Topic-sub

5. VÃ©rification
VÃ©rifiez les logs Docker : docker-compose logs -f

Confirmez la collecte et le stockage des donnÃ©es en accÃ©dant Ã  MongoDB ou en inspectant les topics Kafka.

ğŸ§ª Tester la solution
Lancer manuellement le script Aggregator pour simuler une collecte.

VÃ©rifier que les donnÃ©es apparaissent dans les topics Kafka.

VÃ©rifier que le consumer les insÃ¨re correctement dans MongoDB.

ğŸ“ Structure du projet
bash
Copier
Modifier
.
â”œâ”€â”€ aggregator/           # Service dâ€™ingestion
â”œâ”€â”€ consumer/             # Service de consommation Kafka
â”œâ”€â”€ docker-compose.yml    # Orchestration Docker
â”œâ”€â”€ kafka/                # Configuration Kafka/Zookeeper
â”œâ”€â”€ mongodb/              # DonnÃ©es MongoDB (volumes, init)
â”œâ”€â”€ config/               # Fichiers de config
â””â”€â”€ README.md
âœ¨ Ã€ venir / TODO
IntÃ©gration dâ€™un dashboard de visualisation (ex: Metabase, Grafana)

Monitoring avec Prometheus / Grafana

Tests unitaires pour les microservices

DÃ©ploiement sur Kubernetes (en option)

ğŸ“„ Licence
Ce projet est open source, sous licence MIT.
